name: Shared Build Onprem

on:
  workflow_call:
    inputs:
      app-name:
        required: true
        type: string
      package-json-path:
        required: false
        type: string
        default: 'package.json'
      k8s-namespace:
        required: false
        type: string
        default: 'apps'
      resource-type:
        required: false
        type: string
        default: 'deployment'
      release-tag:
        required: false
        type: string
        description: 'Optional: Manually specify a release tag. If not provided, it will be determined automatically.'
      environment:
        required: true
        type: string
        description: 'Environment to build for (e.g., staging or production)'
    secrets:
      COSIGN_PRIVATE_KEY:
        required: true
      COSIGN_PASSWORD:
        required: true
      STG_HARBOR_REGISTRY_RW_USERNAME:
        required: false
      STG_HARBOR_REGISTRY_RW_PASSWORD:
        required: false
      PROD_HARBOR_REGISTRY_RW_USERNAME:
        required: false
      PROD_HARBOR_REGISTRY_RW_PASSWORD:
        required: false

jobs:
  determine-tag-and-versions:
    runs-on: ubuntu-latest
    env:
          COSIGN_PRIVATE_KEY: ${{ secrets.COSIGN_PRIVATE_KEY }}
          COSIGN_PASSWORD: ${{ secrets.COSIGN_PASSWORD }}
          HARBOR_REGISTRY: 'harbor.harbor.svc.cluster.local'
          IMAGE_NAME: 'bri/${{ github.event.repository.name }}'
    outputs:
      release-tag: ${{ steps.get-tag.outputs.tag }}
      node-version: ${{ steps.extract-versions.outputs.node-version }}
      pnpm-version: ${{ steps.extract-versions.outputs.pnpm-version }}
      timestamp: ${{ steps.timestamp.outputs.value }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Get timestamp
        id: timestamp
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          echo "value=$TIMESTAMP" >> $GITHUB_OUTPUT
          
      - name: Get tag
        id: get-tag
        run: |
          # First try to use the input tag if provided
          if [[ -n "${{ inputs.release-tag }}" ]]; then
            TAG="${{ inputs.release-tag }}"
            echo "Using provided tag: $TAG"
          # Next try to use the release event tag if this is triggered by a release
          elif [[ -n "${{ github.event.release.tag_name }}" ]]; then
            TAG="${{ github.event.release.tag_name }}"
            echo "Using release event tag: $TAG"
          # For staging, use the commit SHA with timestamp
          elif [[ "${{ inputs.environment }}" == "staging" ]]; then
            TAG="${{ github.sha }}-$(date +%Y%m%d%H%M%S)"
            echo "Using staging tag with commit SHA and timestamp: $TAG"
          # Otherwise get the latest tag from git
          else
            TAG=$(git describe --tags --abbrev=0)
            echo "Using latest git tag: $TAG"
          fi
          
          # Remove any 'v' prefix if present
          TAG="${TAG#v}"
          
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Using release tag: $TAG"
          
      - name: Extract versions
        id: extract-versions
        run: |
          # Extract Node version from engines field
          NODE_VERSION=$(cat ${{ inputs.package-json-path }} | grep -o '"node": *"[^"]*"' | cut -d'"' -f4)

          # Extract pnpm version from packageManager field
          PNPM_VERSION=$(cat ${{ inputs.package-json-path }} | grep -o '"packageManager": *"[^"]*"' | cut -d'@' -f2 | tr -d '"')

          # Output for GitHub Actions
          echo "node-version=$NODE_VERSION" >> $GITHUB_OUTPUT
          echo "pnpm-version=$PNPM_VERSION" >> $GITHUB_OUTPUT

          # Also print for debugging
          echo "Extracted Node.js version: $NODE_VERSION"
          echo "Extracted pnpm version: $PNPM_VERSION" 

  build-to-harbor:
    needs: [determine-tag-and-versions]
    runs-on: arc-runners-arc-runner-set
    container:
      image: gcr.io/kaniko-project/executor:v1.22.0-debug
    permissions:
      contents: read
      packages: write
      id-token: write
    outputs:
      digest: ${{ steps.digest.outputs.digest }}
    env:
      KANIKO_CACHE_ARGS: '--cache=true --cache-run-layers=true --cache-ttl=4383h --cache-repo=harbor.harbor.svc.cluster.local/bri-cache/docker --registry-map=index.docker.io=harbor.harbor.svc.cluster.local/dockerhub/library --snapshot-mode=redo --use-new-run --compressed-caching=true'
      NODE_VERSION: ${{ needs.determine-tag-and-versions.outputs.node-version }}
      PNPM_VERSION: ${{ needs.determine-tag-and-versions.outputs.pnpm-version }}
      NODE_ENV: ${{ inputs.environment }}
      HARBOR_USERNAME: ${{ inputs.environment == 'staging' && secrets.STG_HARBOR_REGISTRY_RW_USERNAME || secrets.PROD_HARBOR_REGISTRY_RW_USERNAME }}
      HARBOR_PASSWORD: ${{ inputs.environment == 'staging' && secrets.STG_HARBOR_REGISTRY_RW_PASSWORD || secrets.PROD_HARBOR_REGISTRY_RW_PASSWORD }}
    steps:
      - name: Build and Push Image to Harbor with kaniko
        id: build
        env:
          HARBOR_REGISTRY: 'harbor.harbor.svc.cluster.local'
          IMAGE_NAME: 'bri/${{ github.event.repository.name }}'
          GIT_USERNAME: ${{ github.actor }}
          GIT_PASSWORD: ${{ secrets.GITHUB_TOKEN }}
          RELEASE_TAG: ${{ needs.determine-tag-and-versions.outputs.release-tag }}
          ENV_TAG: ${{ inputs.environment }}
          GITHUB_REPO_LOWERCASE: ${{ github.repository && github.repository || '' }}
        run: |
          cat <<EOF > /kaniko/.docker/config.json
          {
            "auths": {
              "harbor.harbor.svc.cluster.local": {
                "auth": "$(echo -n "$HARBOR_USERNAME:$HARBOR_PASSWORD" | base64 -w0)"
              },
              "ghcr.io": {
                "auth": "$(echo -n "$GIT_USERNAME:$GIT_PASSWORD" | base64 -w0)"
              }
            }
          }
          EOF
          
          # Ensure config.json has correct permissions
          chmod 600 /kaniko/.docker/config.json
          
          # Convert GitHub repository name to lowercase for Docker registry compatibility
          GITHUB_REPO_LOWERCASE=$(echo "$GITHUB_REPOSITORY" | tr '[:upper:]' '[:lower:]')
          
          echo "Building and pushing with tag: $RELEASE_TAG"
          echo "Environment: $ENV_TAG"
          echo "Using Node.js version: $NODE_VERSION"
          echo "Using pnpm version: $PNPM_VERSION"
          echo "Using lowercase repository name: $GITHUB_REPO_LOWERCASE"

          # Build image with appropriate tags based on environment
          if [[ "$ENV_TAG" == "staging" ]]; then
            # For staging, add timestamp-based tag, staging tag, and commit SHA tag
            /kaniko/executor --dockerfile="./Dockerfile" \
              --context="${{ github.repositoryUrl }}#${{ github.ref }}#${{ github.sha }}"  \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG-$RELEASE_TAG" \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG" \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:${{ github.sha }}" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:$ENV_TAG-$RELEASE_TAG" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:$ENV_TAG" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:${{ github.sha }}" \
              --image-name-with-digest-file=/kaniko/digests.txt \
              --build-arg NODE_ENV=${{ env.NODE_ENV }} \
              --build-arg NODE_VERSION=${{ env.NODE_VERSION }} \
              --build-arg PNPM_VERSION=${{ env.PNPM_VERSION }} \
              --label "org.opencontainers.image.authors=${{ github.actor }}" \
              ${{ env.KANIKO_CACHE_ARGS }} \
              --insecure \
              --insecure-pull \
              --skip-tls-verify \
              --skip-tls-verify-pull \
              --push-retry 5
          else
            # For production, add version tag, production tag, and commit SHA tag
            /kaniko/executor --dockerfile="./Dockerfile" \
              --context="${{ github.repositoryUrl }}#${{ github.ref }}#${{ github.sha }}"  \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:$RELEASE_TAG" \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG" \
              --destination="$HARBOR_REGISTRY/$IMAGE_NAME:${{ github.sha }}" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:$RELEASE_TAG" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:$ENV_TAG" \
              --destination="ghcr.io/$GITHUB_REPO_LOWERCASE:${{ github.sha }}" \
              --image-name-with-digest-file=/kaniko/digests.txt \
              --build-arg NODE_ENV=${{ env.NODE_ENV }} \
              --build-arg NODE_VERSION=${{ env.NODE_VERSION }} \
              --build-arg PNPM_VERSION=${{ env.PNPM_VERSION }} \
              --label "org.opencontainers.image.authors=${{ github.actor }}" \
              ${{ env.KANIKO_CACHE_ARGS }} \
              --insecure \
              --insecure-pull \
              --skip-tls-verify \
              --skip-tls-verify-pull \
              --push-retry 5
          fi

      - name: Extract digest
        id: digest
        run: |
          DIGEST=$(cat /kaniko/digests.txt | grep "harbor.harbor.svc.cluster.local" | head -n1 | cut -d'@' -f2)
          echo "digest=$DIGEST" >> $GITHUB_OUTPUT
          echo "digest=$DIGEST"

  sign-images:
    needs: [determine-tag-and-versions, build-to-harbor]
    runs-on: arc-runners-arc-runner-set
    permissions:
      contents: read
      id-token: write
      packages: write
    env:
      HARBOR_USERNAME: ${{ inputs.environment == 'staging' && secrets.STG_HARBOR_REGISTRY_RW_USERNAME || secrets.PROD_HARBOR_REGISTRY_RW_USERNAME }}
      HARBOR_PASSWORD: ${{ inputs.environment == 'staging' && secrets.STG_HARBOR_REGISTRY_RW_PASSWORD || secrets.PROD_HARBOR_REGISTRY_RW_PASSWORD }}
    steps:
      - name: Install cosign
        uses: sigstore/cosign-installer@v3.8.2

      - name: Sign Harbor images
        env:
          COSIGN_PRIVATE_KEY: ${{ secrets.COSIGN_PRIVATE_KEY }}
          COSIGN_PASSWORD: ${{ secrets.COSIGN_PASSWORD }}
          HARBOR_REGISTRY: 'harbor.harbor.svc.cluster.local'
          IMAGE_NAME: 'bri/${{ github.event.repository.name }}'
          HARBOR_DIGEST: ${{ needs.build-to-harbor.outputs.digest }}
        run: |
          echo "${COSIGN_PRIVATE_KEY}" > /tmp/cosign.key

          # Setup Docker config
          mkdir -p ~/.docker
          
          cat <<EOF > ~/.docker/config.json
          {
            "auths": {
              "harbor.harbor.svc.cluster.local": {
                "auth": "$(echo -n "$HARBOR_USERNAME:$HARBOR_PASSWORD" | base64 -w0)"
              }
            }
          }
          EOF
          
          # Ensure config.json has correct permissions
          chmod 600 ~/.docker/config.json

          if [ -n "$HARBOR_DIGEST" ]; then
            cosign sign --key /tmp/cosign.key --yes --allow-insecure-registry=true "$HARBOR_REGISTRY/$IMAGE_NAME@$HARBOR_DIGEST"
          fi

  check-k8s-deployment:
    needs: [determine-tag-and-versions, sign-images, build-to-harbor]
    runs-on: arc-runners-arc-runner-set
    permissions:
      contents: read
      id-token: write
    steps:
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
      
      - name: Check deployment status
        env:
          KUBE_NAMESPACE: ${{ inputs.k8s-namespace }}
          RESOURCE_TYPE: ${{ inputs.resource-type }}
          APP_NAME: ${{ inputs.app-name }}
          RELEASE_TAG: ${{ needs.determine-tag-and-versions.outputs.release-tag }}
          ENV_TAG: ${{ inputs.environment }}
          IMAGE_TAG: ${{ github.sha }}
          HARBOR_REGISTRY: 'harbor.harbor.svc.cluster.local'
          IMAGE_NAME: 'bri/${{ github.event.repository.name }}'
          HARBOR_DIGEST: ${{ needs.build-to-harbor.outputs.digest }}
          MAX_RETRIES: 30
          RETRY_INTERVAL: 10
        run: |
          # Check if service account token exists
          if [ -f /var/run/secrets/kubernetes.io/serviceaccount/token ]; then
            echo "Using in-cluster service account token"
          else
            echo "No service account token found - ensure your runner has the correct permissions"
            exit 1
          fi
          
          # Convert resource type to lowercase for consistency
          RESOURCE_TYPE=$(echo "$RESOURCE_TYPE" | tr '[:upper:]' '[:lower:]')
          echo "Checking for resource type: $RESOURCE_TYPE in namespace: $KUBE_NAMESPACE"
          
          # Wait for deployment to use new image
          echo "Checking if $RESOURCE_TYPE is using the new image with digest $HARBOR_DIGEST or tag $RELEASE_TAG"
          
          retry_count=0
          deployed=false
          
          while [ $retry_count -lt $MAX_RETRIES ] && [ "$deployed" = false ]; do
            # Check if the resource exists and is using our image
            if kubectl get $RESOURCE_TYPE -n $KUBE_NAMESPACE $APP_NAME 2>/dev/null; then
              # Get current image being used
              CURRENT_IMAGE=$(kubectl get $RESOURCE_TYPE -n $KUBE_NAMESPACE $APP_NAME -o jsonpath='{.spec.template.spec.containers[0].image}')
              
              echo "Current image: $CURRENT_IMAGE"
              if [[ "$ENV_TAG" == "staging" ]]; then
                echo "Expected image by tag: $HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG-$RELEASE_TAG"
                echo "Expected image by env: $HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG"
              else
                echo "Expected image by tag: $HARBOR_REGISTRY/$IMAGE_NAME:$RELEASE_TAG"
                echo "Expected image by env: $HARBOR_REGISTRY/$IMAGE_NAME:$ENV_TAG"
              fi
              echo "Expected image by hash: $HARBOR_REGISTRY/$IMAGE_NAME:$IMAGE_TAG or with digest $HARBOR_DIGEST"
              
              # Check if deployment is using our image (either by environment, release tag, commit hash, or digest)
              if [[ "$CURRENT_IMAGE" == *"$ENV_TAG"* ]] || [[ "$CURRENT_IMAGE" == *"$RELEASE_TAG"* ]] || [[ "$CURRENT_IMAGE" == *"$IMAGE_TAG"* ]] || [[ -n "$HARBOR_DIGEST" && "$CURRENT_IMAGE" == *"$HARBOR_DIGEST"* ]]; then
                echo "$RESOURCE_TYPE is using the new image!"
                
                # Check if deployment is ready
                READY_REPLICAS=$(kubectl get $RESOURCE_TYPE -n $KUBE_NAMESPACE $APP_NAME -o jsonpath='{.status.readyReplicas}')
                REPLICAS=$(kubectl get $RESOURCE_TYPE -n $KUBE_NAMESPACE $APP_NAME -o jsonpath='{.status.replicas}')
                
                if [ "$READY_REPLICAS" == "$REPLICAS" ] && [ "$READY_REPLICAS" -gt 0 ]; then
                  echo "All pods are reported ready at resource level: $READY_REPLICAS/$REPLICAS"
                  
                  # Additional check to verify actual pod readiness
                  PODS=$(kubectl get pods -n $KUBE_NAMESPACE -l app=$APP_NAME -o name)
                  all_pods_ready=true
                  all_pods_have_new_image=true
                  
                  for POD in $PODS; do
                    POD_NAME=$(echo $POD | cut -d'/' -f2)
                    POD_STATUS=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{.status.phase}')
                    CONTAINER_READY=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{.status.containerStatuses[0].ready}')
                    
                    # Check the actual running image in each pod
                    POD_IMAGE=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{.spec.containers[0].image}')
                    echo "Pod $POD_NAME is running image: $POD_IMAGE"
                    
                    # Verify the pod is using the new image
                    if [[ "$POD_IMAGE" != *"$ENV_TAG"* ]] && [[ "$POD_IMAGE" != *"$RELEASE_TAG"* ]] && [[ "$POD_IMAGE" != *"$IMAGE_TAG"* ]] && [[ -n "$HARBOR_DIGEST" && "$POD_IMAGE" != *"$HARBOR_DIGEST"* ]]; then
                      echo "Pod $POD_NAME is not running the expected image!"
                      all_pods_have_new_image=false
                    fi
                    
                    # Check if pod is truly ready - enhanced to better handle StatefulSets
                    if [[ "$POD_STATUS" != "Running" ]] || [[ "$CONTAINER_READY" != "true" ]]; then
                      echo "Pod $POD_NAME is not ready. Status: $POD_STATUS, Container Ready: $CONTAINER_READY"
                      all_pods_ready=false
                      
                      # Show pod conditions
                      echo "--- Pod Conditions ---"
                      kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{range .status.conditions[*]}{.type}{"\t"}{.status}{"\n"}{end}'
                      
                      # Check for init containers that might still be running
                      INIT_CONTAINERS=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{.spec.initContainers[*].name}' 2>/dev/null)
                      if [ -n "$INIT_CONTAINERS" ]; then
                        echo "--- Init Containers Status ---"
                        kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{range .status.initContainerStatuses[*]}{.name}{"\t"}{.state}{"\n"}{end}'
                      fi
                      
                      # Get container status details
                      echo "--- Container Status Details ---"
                      kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state}{"\n"}{end}'
                      
                      # Check if container is in waiting state with specific reason
                      WAITING_REASON=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null)
                      if [ -n "$WAITING_REASON" ]; then
                        echo "Container waiting reason: $WAITING_REASON"
                        # If container is creating/initializing, we need to wait
                        if [[ "$WAITING_REASON" == "ContainerCreating" ]] || [[ "$WAITING_REASON" == "PodInitializing" ]]; then
                          all_pods_ready=false
                        fi
                      fi
                      
                      # Get logs if possible
                      echo "--- Container logs ---"
                      kubectl logs $POD -n $KUBE_NAMESPACE --tail=50 || echo "Cannot fetch logs"
                    else
                      # Additional check for startup probe or statefulset readiness
                      # Even if container is "ready", check if all conditions are true
                      READY_CONDITIONS=$(kubectl get $POD -n $KUBE_NAMESPACE -o jsonpath='{range .status.conditions[?(@.status=="False")]}{.type}{"\n"}{end}')
                      if [ -n "$READY_CONDITIONS" ]; then
                        echo "Pod $POD_NAME has some conditions not ready: $READY_CONDITIONS"
                        all_pods_ready=false
                      else
                        echo "Pod $POD_NAME is running and truly ready."
                      fi
                    fi
                  done
                  
                  if [ "$all_pods_ready" = true ] && [ "$all_pods_have_new_image" = true ]; then
                    deployed=true
                    echo "All pods are verified to be actually ready, running, and using the new image."
                  else
                    if [ "$all_pods_have_new_image" = false ]; then
                      echo "Not all pods are running the expected image version. Waiting for rolling update to complete."
                    else
                      echo "Resource reports ready but actual pod check failed. Waiting for pods to be truly ready."
                    fi
                  fi
                fi
              fi
            else
              echo "$RESOURCE_TYPE $APP_NAME not found in namespace $KUBE_NAMESPACE"
            fi
            
            if [ "$deployed" = false ]; then
              retry_count=$((retry_count+1))
              echo "Attempt $retry_count/$MAX_RETRIES. Waiting $RETRY_INTERVAL seconds before retrying..."
              sleep $RETRY_INTERVAL
            fi
          done
          
          if [ "$deployed" = true ]; then
            echo "$RESOURCE_TYPE verified successfully!"
            exit 0
          else
            echo "Failed to verify $RESOURCE_TYPE after $MAX_RETRIES attempts"
            exit 1
          fi 